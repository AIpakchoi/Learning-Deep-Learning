{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "* constant tensor `tf.constant()`\n",
    "    * Value of tensor never changes, hence *constant*.\n",
    "    * `tf.constant(1234)` is a 0-dimensional int32 tensor\n",
    "    * `tf.constant([1,2,3,4])` is a 4-dimensional int32 tensor\n",
    "\n",
    "Sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Create TensorFlow object called tensor\n",
    "hello_constant = tf.constant('Hello World!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session\n",
    "* An environment for running a graph. In charge of allocating the operations to GPU(s) and/or CPU(s).\n",
    "\n",
    "Continuing our example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    output = sess.run(hello_constant)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "\n",
    "* `tf.placeholder()`: returns a tensor that gets it’s value from data passed to the `tf.session.run()` function, allowing you to set the input right before the session runs.\n",
    "* `feed_dict`: Use the feed_dict parameter in tf.session.run() to set the placeholder tensor. \n",
    "\n",
    "Example: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.string)\n",
    "b = tf.placeholder(tf.int32)\n",
    "c = tf.placeholder(tf.float32)\n",
    "with tf.Session() as sess:\n",
    "    output = sess.run(a, feed_dict={a: 'hi', b: 23, c: 32.0})\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also works if you feed it only `{a: 'hi'}`, i.e. the relevant placeholder value(s).\n",
    "\n",
    "\n",
    "## Maths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 6, 10, 2]\n"
     ]
    }
   ],
   "source": [
    "# Add, subtract, multiply and divide operations\n",
    "# sub and mul has been removed in v1.0.1\n",
    "add = tf.add(5, 2) # 7\n",
    "# sub = tf.sub(10, 4) # 6\n",
    "# mul = tf.mul(2, 5)  # 10\n",
    "sub = tf.subtract(10, 4) # 6\n",
    "mul = tf.multiply(2, 5)  # 10\n",
    "div = tf.div(10, 5) # 2\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    output = [sess.run(add), sess.run(sub), sess.run(mul), \n",
    "              sess.run(div)]\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[TF Math documentation](https://www.tensorflow.org/versions/r0.11/api_docs/python/math_ops.html)\n",
    "\n",
    "## Variables\n",
    "\n",
    "* `tf.Variable()` function creates a tensor with an initial value that can be modified later, much like a normal Python variable. This tensor stores it’s state in the session, so you must use the `tf.initialize_all_variables()` function to initialize the state of the tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialisation\n",
    "\n",
    "def variables():\n",
    "    output = None\n",
    "    \n",
    "    x = tf.Variable([1, 2, 3, 4])\n",
    "    \n",
    "    # Initialise all variables\n",
    "    # init = tf.initialize_all_variables()\n",
    "    \"\"\"initialize_all_variables (from tensorflow.python.ops.variables) is deprecated \n",
    "    and will be removed after 2017-03-02.\"\"\"\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        output = sess.run(x)\n",
    "    \n",
    "    return output\n",
    "\n",
    "variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.80277288, -1.83862185],\n",
       "       [ 1.24173021,  3.4572463 ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "def logits():\n",
    "    output = None\n",
    "    x_data = [[1.0, 2.0], [2.5, 6.3]]\n",
    "    test_weights = [[-0.3545495, -0.17928936], [-0.63093454, 0.74906588]]\n",
    "    class_size = 2\n",
    "    \n",
    "    \n",
    "    x = tf.placeholder(tf.float32, shape=(2, 2))\n",
    "    weights = tf.Variable(test_weights)\n",
    "    biases = tf.Variable(tf.zeros([class_size]))\n",
    "    \n",
    "    # ToDo: Implement wx + b in TensorFlow\n",
    "    logits = tf.matmul(weights, x)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        output = sess.run(logits, feed_dict={x: x_data})\n",
    "        \n",
    "    return output\n",
    "\n",
    "logits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before update:\n",
      "[[ 0.7310586 ]\n",
      " [ 0.98201376]]\n",
      "After update:\n",
      "[[ 0.99998331]\n",
      " [ 0.999089  ]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression, modified\n",
    "\n",
    "\n",
    "output = None\n",
    "x_data = np.array([1., 2.]).reshape((2, 1))\n",
    "test_weights = [[1., 0.], [2., 1.]]\n",
    "class_size = 2\n",
    "\n",
    "\n",
    "#     x = tf.placeholder(tf.float32)\n",
    "x = tf.placeholder(tf.float32, shape=(2, 1))\n",
    "w = tf.Variable(test_weights)\n",
    "b = tf.Variable(tf.zeros([class_size]))\n",
    "b = tf.Variable([[0.], [0.]])\n",
    "# all op has to be created and run!\n",
    "b_update = tf.assign(b, [[10.], [3.]])\n",
    "\n",
    "\n",
    "# Implement wx + b in TensorFlow\n",
    "logits = ((tf.matmul(w, x) + b) )\n",
    "logits = tf.sigmoid(logits)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    output = sess.run(logits, feed_dict={x: x_data})\n",
    "    print('Before update:\\n{}'.format(output))\n",
    "    sess.run(b_update) # crititcal!\n",
    "    output = sess.run(logits, feed_dict={x: x_data})\n",
    "    print('After update:\\n{}'.format(output))\n",
    "    tf.Print(logits, [logits], message='This is logits:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Print_1:0' shape=(2, 1) dtype=float32>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Print(logits, [logits])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sigmoid_14:0\", shape=(2, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n",
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = tf.Variable([1, 2])\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "print(x.eval())\n",
    "\n",
    "# x.assign(1)\n",
    "sess.run(tf.assign(x, [3, 4]))\n",
    "print(x.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = [[0]\n",
      " [0]\n",
      " [0]]\n",
      "c = [[0 0 0]]\n",
      "b+c = [[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable([[1], [4], [10]])\n",
    "b = tf.Variable(tf.zeros([3, 1], dtype=np.int32))\n",
    "c = tf.Variable(tf.zeros([1, 3], dtype=np.int32))\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    output = sess.run(b + c)\n",
    "    print('b = {}'.format(sess.run(b)))\n",
    "    print('c = {}'.format(sess.run(c)))\n",
    "    print('b+c = {}'.format(sess.run(b+c))) # uses broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Difference between placeholder and Variable (and Tensor)\n",
    "[Link](http://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable)\n",
    "\n",
    "- Placeholder is used to store the input training data and given labels. The Variables are used to store the weights to be trained. In other words, **placeholder**s are used to train **Variable**s. \n",
    "- Variables has to be initialized while placeholder just need to be specified datatype and size.\n",
    "\n",
    "### How about Tensor itself?\n",
    "- A Tensor object is a symbolic handle to the result of an operation, but does not actually hold the values of the operation's output.\n",
    "- Therefore it is impossible to check the value of a tensor without running the graph (a collection of operations on tensors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.60517025  2.30258512]\n",
      "[ 0.90909088  0.09090908]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([np.log(100), np.log(10)])\n",
    "b = tf.nn.softmax(a)\n",
    "print(tf.Session().run(a))\n",
    "print(tf.Session().run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Turns logits into probabilities that sum to 1.\n",
    "* `tf.nn.softmax()`.\n",
    "\n",
    "Example of how it works:\n",
    "\n",
    "```\n",
    "# logits is a one-dimensional array with 3 elements\n",
    "logits = [1.0, 2.0, 3.0]\n",
    "# softmax will return a one-dimensional array with 3 elements\n",
    "print softmax(logits)\n",
    "\n",
    "[ 0.09003057  0.24472847  0.66524096]\n",
    "\n",
    "# logits is a two-dimensional array\n",
    "logits = np.array([\n",
    "    [1, 2, 3, 6],\n",
    "    [2, 4, 5, 6],\n",
    "    [3, 8, 7, 6]])\n",
    "# softmax will return a two-dimensional array with the same shape\n",
    "print softmax(logits)\n",
    "\n",
    "\n",
    "[\n",
    "    [ 0.09003057  0.00242826  0.01587624  0.33333333]\n",
    "    [ 0.24472847  0.01794253  0.11731043  0.33333333]\n",
    "    [ 0.66524096  0.97962921  0.86681333  0.33333333]\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax function in ram Python\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    # TODO: Compute and return softmax(x)\n",
    "    # S(y_i) = (e**(y_i) / sum_over_j(e**y_j))\n",
    "    # sum over last dim\n",
    "    return np.exp(x) / np.reshape(np.sum(np.exp(x), axis=-1), list(x.shape[:-1]) + [1]) \n",
    "    # sum over the first dimention\n",
    "#     return np.exp(x) / np.sum(np.exp(x), axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00626879,  0.01704033,  0.04632042,  0.93037047],\n",
       "       [ 0.01203764,  0.08894682,  0.24178252,  0.65723302],\n",
       "       [ 0.00446236,  0.66227241,  0.24363641,  0.08962882]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = np.array([\n",
    "    [1, 2, 3, 6],\n",
    "    [2, 4, 5, 6],\n",
    "    [3, 8, 7, 6]])\n",
    "softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's some elegant Numpy code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.9009009 ,  0.09009008,  0.00900901], dtype=float32)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax with TF\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    logit_data = np.log([10.0, 1.0, 0.1])\n",
    "    logits = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # ToDo: Calculate the softmax of the logits\n",
    "    softmax = tf.nn.softmax(logits)    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # ToDo: Feed in the logits data\n",
    "        output = sess.run(softmax, feed_dict={logits: logit_data})\n",
    "\n",
    "    return output\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Scaling and Softmax\n",
    "* When you divide all the logits by e.g. 10, the probabilities get closer to the uniform distribution.\n",
    "* When you multiply all the logits by e.g. 10, the probabilities get closer to 0.0 or 1.0.\n",
    "\n",
    "## One-Hot Encodings\n",
    "* Vectors with one 1.0 and 0.0 everywhere else.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLUs: f(x) = max(0,x)\n",
    "*Adding nonlinearities*\n",
    "\n",
    "A Rectified linear unit (ReLU) is type of **activation function** that is defined as `f(x) = max(0, x)`. The function returns 0 if `x` is negative, otherwise it returns `x`. TensorFlow provides the ReLU function as `tf.nn.relu()`, as shown below.\n",
    "\n",
    "![](images/relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden Layer with ReLU activation function\n",
    "hidden_layer = tf.add(tf.matmul(features, weights), biases)\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "output = tf.add(tf.matmul(hidden_layer, weights), biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code applies the `tf.nn.relu()` function to the `hidden_layer`, effectively turning off any negative weights and acting like an on/off switch. Adding additional layers, like the output layer, after an activation function turns the model into a nonlinear function. This nonlinearity allows the network to solve more complex problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting how you just add `hidden_layer=tf.nn.relu(hidden_layer)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "def run():\n",
    "    output = None\n",
    "    hidden_layer_weights = [\n",
    "        [0.1, 0.2, 0.4],\n",
    "        [0.4, 0.6, 0.6],\n",
    "        [0.5, 0.9, 0.1],\n",
    "        [0.8, 0.2, 0.8]]\n",
    "    out_weights = [\n",
    "        [0.1, 0.6],\n",
    "        [0.2, 0.1],\n",
    "        [0.7, 0.9]]\n",
    "\n",
    "    # Weights and biases\n",
    "    weights = [\n",
    "        tf.Variable(hidden_layer_weights),\n",
    "        tf.Variable(out_weights)]\n",
    "    biases = [\n",
    "        tf.Variable(tf.zeros(3)),\n",
    "        tf.Variable(tf.zeros(2))]\n",
    "\n",
    "    # Input\n",
    "    features = tf.Variable([[1.0, 2.0, 3.0, 4.0], \n",
    "                            [-1.0, -2.0, -3.0, -4.0], \n",
    "                            [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "    # Model\n",
    "    hidden_layer = tf.matmul(features, weights[0]) + biases[0]\n",
    "    # ToDo: Apply activation using a single Relu\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    logits = tf.matmul(hidden_layer, weights[1]) + biases[1]\n",
    "\n",
    "    # Calculate logits\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        output = sess.run(logits)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.11000013,   8.44000053],\n",
       "       [  0.        ,   0.        ],\n",
       "       [ 24.01000214,  38.23999786]], dtype=float32)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function zeros in module tensorflow.python.ops.array_ops:\n",
      "\n",
      "zeros(shape, dtype=tf.float32, name=None)\n",
      "    Creates a tensor with all elements set to zero.\n",
      "    \n",
      "    This operation returns a tensor of type `dtype` with shape `shape` and\n",
      "    all elements set to zero.\n",
      "    \n",
      "    For example:\n",
      "    \n",
      "    ```python\n",
      "    tf.zeros([3, 4], tf.int32) ==> [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "      shape: Either a list of integers, or a 1-D `Tensor` of type `int32`.\n",
      "      dtype: The type of an element in the resulting `Tensor`.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A `Tensor` with all elements set to zero.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.zeros)\n",
    "# tf.Session().run(tf.zeros((4, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True)\n",
    "# Udacity version included reshape=False but this got the \n",
    "# 'unexpected keyword' error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick peep into data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9\n",
       "0  0  0  0  0  0  0  0  1  0  0\n",
       "1  0  0  0  1  0  0  0  0  0  0\n",
       "2  0  0  0  0  1  0  0  0  0  0\n",
       "3  0  0  0  0  0  0  1  0  0  0\n",
       "4  0  1  0  0  0  0  0  0  0  0"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.DataFrame(mnist.train._labels).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 784)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train._images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Learning Parameters\n",
    "## Usually we have to find these.\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "n_input = 784  # MNIST data input (img shape: 28*28), i.e., input feature dimension\n",
    "n_classes = 10  # MNIST total classes (0-9 digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_layer = 256 # layer number of features (width of a layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and biases\n",
    "\n",
    "**Note:** \n",
    "All matrix is of (sample x feature) size, or (input x output) size\n",
    "- Every row is a sample vector (input)\n",
    "- Every col is a feature vector (output)\n",
    "\n",
    "Broadcast rules:\n",
    "```\n",
    "m samples, k features\n",
    "(m, k) + (m,) --> (m, k) + (1, m) --> broadcast failure!\n",
    "(m, k) + (k,) --> (m, k) + (1, k) --> broadcast success!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "# random initialization\n",
    "weights = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_input, n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_layer, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden_layer': tf.Variable(tf.random_normal([n_hidden_layer])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "# the size of the samples is TBD, ie `None`\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "x_flat = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape\n",
    "- tf.shape() creates an op. It has to be run to return the real shape of the tensor\n",
    "- x.get_shape() gets the real shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Shape_56:0' shape=(2,) dtype=int32>"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(weights['hidden_layer']) # shape of the op, means 2-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([784, 256], dtype=int32)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Session().run(tf.shape(weights['hidden_layer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(784), Dimension(256)])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((weights['hidden_layer'])._variable).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(784), Dimension(256)])"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['hidden_layer'].get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[784, 256]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to normal python shape\n",
    "weights['hidden_layer'].get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 784, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "x_flat = x\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# The MNIST data is made up of 28px by 28px images with a single channel. \n",
    "# `tf.reshape` reshapes a batch of 28*28 pixels, x, to a batch of 784 pixels.\n",
    "x_flat = tf.reshape(x, [-1, n_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, 28, 28, 1])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# The MNIST data is made up of 28px by 28px images with a single channel. \n",
    "# `tf.reshape` reshapes a batch of 28*28 pixels, x, to a batch of 784 pixels.\n",
    "x_flat = tf.reshape(x, [-1, n_input])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hidden layer with RELU activation\n",
    "layer_1 = tf.add(tf.matmul(x_flat, weights['hidden_layer']), biases['hidden_layer'])\n",
    "layer_1 = tf.nn.relu(layer_1)\n",
    "# Output layer with linear activation\n",
    "logits = tf.matmul(layer_1, weights['out']) + biases['out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*So we're putting RELUs in between layers with weights and biases in them to allow for more complexity. Here we have two layers sandwiching a ReLU.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "(100, 784)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "print(batch_size)\n",
    "print(batch_x.shape)\n",
    "print(batch_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\t0\tcost\t35.951\n",
      "epoch\t1\tcost\t24.361\n",
      "epoch\t2\tcost\t17.062\n",
      "epoch\t3\tcost\t14.131\n",
      "epoch\t4\tcost\t15.133\n",
      "epoch\t5\tcost\t12.878\n",
      "epoch\t6\tcost\t7.694\n",
      "epoch\t7\tcost\t9.774\n",
      "epoch\t8\tcost\t11.218\n",
      "epoch\t9\tcost\t18.743\n",
      "epoch\t10\tcost\t6.224\n",
      "epoch\t11\tcost\t5.963\n",
      "epoch\t12\tcost\t6.953\n",
      "epoch\t13\tcost\t8.625\n",
      "epoch\t14\tcost\t7.692\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "display_step = 1\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        total_c = 0\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x_flat: batch_x, y: batch_y})\n",
    "            total_c += c\n",
    "        if (epoch % display_step == 0):\n",
    "            print('epoch\\t{}\\tcost\\t{:.3f}'.format(epoch, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./train-images-idx3-ubyte.gz\n",
      "Extracting ./train-labels-idx1-ubyte.gz\n",
      "Extracting ./t10k-images-idx3-ubyte.gz\n",
      "Extracting ./t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 183.440410611\n",
      "Epoch: 0002 cost= 40.923661626\n",
      "Epoch: 0003 cost= 25.367347529\n",
      "Epoch: 0004 cost= 17.637425760\n",
      "Epoch: 0005 cost= 12.756124606\n",
      "Epoch: 0006 cost= 9.527275549\n",
      "Epoch: 0007 cost= 6.966637792\n",
      "Epoch: 0008 cost= 5.269151712\n",
      "Epoch: 0009 cost= 3.955956824\n",
      "Epoch: 0010 cost= 2.966537620\n",
      "Epoch: 0011 cost= 2.245004738\n",
      "Epoch: 0012 cost= 1.623642844\n",
      "Epoch: 0013 cost= 1.213023317\n",
      "Epoch: 0014 cost= 1.068061601\n",
      "Epoch: 0015 cost= 0.871373281\n",
      "Optimization Finished!\n",
      "Accuracy: 0.9475\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "A Multilayer Perceptron implementation example using TensorFlow library.\n",
    "This example is using the MNIST database of handwritten digits\n",
    "(http://yann.lecun.com/exdb/mnist/)\n",
    "\n",
    "Author: Aymeric Damien\n",
    "Project: https://github.com/aymericdamien/TensorFlow-Examples/\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\".\", one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "Dropout is a regularization technique for reducing overfitting. The technique temporarily drops units (Artificial Neurons) from the network, along with all its incoming and outgoing connections as shown in Figure 1.\n",
    "*Presumably it then drops those units if it obtains better results when dropping those units.*\n",
    "\n",
    "`tf.nn.dropout()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.float32) # probability to keep units\n",
    "\n",
    "hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])\n",
    "hidden_layer = tf.nn.relu(hidden_layer)\n",
    "hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "\n",
    "logits = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first parameter, hidden_layer, is the tensor that is regularized using dropout.\n",
    "\n",
    "The second parameter, keep_prob, is the probability of keeping (i.e. not dropping) any given unit.\n",
    "\n",
    "keep_prob allows you to adjust the number of units to drop. In order to compensate for dropped units, tf.nn.dropout() multiplies all units that are kept (i.e. not dropped) by 1/keep_prob.\n",
    "\n",
    "During training, a good starting value for keep_prob is 0.5.\n",
    "\n",
    "During testing, use a keep_prob value of 1.0 to keep all units and maximize the power of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added: \n",
    "* `hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)`\n",
    "and \n",
    "* `feed_dict` portion of\n",
    "`output = sess.run(logits, feed_dict={keep_prob: 0.5})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.10000002,   6.60000038],\n",
       "       [  0.11200001,   0.67200011],\n",
       "       [  4.71999979,  28.31999969]], dtype=float32)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Solution is available in the other \"solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def run():\n",
    "    output = None\n",
    "    hidden_layer_weights = [\n",
    "        [0.1, 0.2, 0.4],\n",
    "        [0.4, 0.6, 0.6],\n",
    "        [0.5, 0.9, 0.1],\n",
    "        [0.8, 0.2, 0.8]]\n",
    "    out_weights = [\n",
    "        [0.1, 0.6],\n",
    "        [0.2, 0.1],\n",
    "        [0.7, 0.9]]\n",
    "\n",
    "    # Weights and biases\n",
    "    weights = [\n",
    "        tf.Variable(hidden_layer_weights),\n",
    "        tf.Variable(out_weights)]\n",
    "    biases = [\n",
    "        tf.Variable(tf.zeros(3)),\n",
    "        tf.Variable(tf.zeros(2))]\n",
    "        \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Input\n",
    "    features = tf.Variable([[0.0, 2.0, 3.0, 4.0], \n",
    "                            [0.1, 0.2, 0.3, 0.4], \n",
    "                            [11.0, 12.0, 13.0, 14.0]])\n",
    "\n",
    "    # Model\n",
    "    hidden_layer = tf.matmul(features, weights[0]) + biases[0]\n",
    "    hidden_layer = tf.nn.relu(hidden_layer)\n",
    "    # Add dropout\n",
    "    hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "    \n",
    "    logits = tf.matmul(hidden_layer, weights[1]) + biases[1]\n",
    "\n",
    "    # Calculate logits\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        output = sess.run(logits, feed_dict={keep_prob: 0.5})\n",
    "\n",
    "    return output\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution layer\n",
    "* `tf.nn.conv2d()`\n",
    "    * Computes convolution. TensorFlow uses a stride for each input dimension, [batch, input_height, input_width, input_channels].\n",
    "* `tf.nn.bias_add()`\n",
    "    * adds a 1-d bias to the last dimension in a matrix.\n",
    "    \n",
    "\n",
    "\n",
    "You'll focus on changing input_height and input_width while setting batch and input_channels to 1. The input_height and input_width strides are for striding the filter over input. In the example code, I'm using a stride of 2 with 5x5 filter over input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output depth\n",
    "k_output = 64\n",
    "\n",
    "# Image Properties\n",
    "image_width = 10\n",
    "image_height = 10\n",
    "color_channels = 3\n",
    "\n",
    "# Convolution filter\n",
    "filter_size_width = 5\n",
    "filter_size_height = 5\n",
    "\n",
    "# Input/Image\n",
    "input = tf.placeholder(\n",
    "    tf.float32,\n",
    "    # NHWC order [batch, height, width, channels]\n",
    "    shape=[None, image_width, image_height, color_channels])\n",
    "\n",
    "# Weight and bias\n",
    "weight = tf.Variable(tf.truncated_normal(\n",
    "    [filter_size_width, filter_size_height, color_channels, k_output]))\n",
    "bias = tf.Variable(tf.zeros(k_output))\n",
    "\n",
    "# Apply Convolution\n",
    "# strides must take 1-D of length 4 Must have strides[0] = strides[3] = 1. \n",
    "# For the most common case of the same horizontal and vertices strides, \n",
    "# strides = [1, stride, stride, 1].\n",
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "# Add bias\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "# Apply activation function\n",
    "conv_layer = tf.nn.relu(conv_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 10, 10, 3)\n",
      "(5, 5, 3, 64)\n",
      "(?, 5, 5, 64)\n"
     ]
    }
   ],
   "source": [
    "print(input.get_shape())\n",
    "print(weight.get_shape())\n",
    "print(conv_layer.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max Pooling\n",
    "\n",
    "`tf.nn.max_pool()`\n",
    "\n",
    "The image above is an example of max pooling with a 2x2 filter and stride of 2. The four 2x2 colors represent each time the filter was applied to find the maximum value.\n",
    "\n",
    "* **Benefits of max pooling**: reduces the size of the input, and allow the neural network to focus on only the most important elements. \n",
    "* **Method**: Max pooling does this by only retaining the maximum value for each filtered area, and removing the remaining values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')\n",
    "conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "conv_layer = tf.nn.relu(conv_layer)\n",
    "# Apply Max Pooling\n",
    "conv_layer = tf.nn.max_pool(\n",
    "    conv_layer,\n",
    "    ksize=[1, 2, 2, 1],  # size of window\n",
    "    strides=[1, 2, 2, 1],# size of stride\n",
    "    padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tf.nn.max_pool() function performs max pooling with the ksize parameter as the size of the filter and the strides parameter as the length of the stride. 2x2 filters with a stride of 2x2 are common in practice.\n",
    "\n",
    "The ksize and strides parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor ([batch, height, width, channels]). For both ksize and strides, the batch and channel dimensions are typically set to 1."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
