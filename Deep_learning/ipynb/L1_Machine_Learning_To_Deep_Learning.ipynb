{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L1 Machine Learning to Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is now a part of many branches of computer science, from perception to NLP to computer vision. It gives computer and data scientists a common language and even a common set of tools.\n",
    "\n",
    "Neural networks were fringe in 2000-2010. They only became popular again in recent years because of an increase in the volume of data and because cheap, fast GPUs increased computational power.\n",
    "\n",
    "\n",
    "## Classification\n",
    "Course will focus on problem of classification. Classification paves the way for regression, reinforcement learning, detection and ranking. See two examples below.\n",
    "\n",
    "Example 1: Detection: Self-driving car detecting pedestrians from an image\n",
    "* Use a binary pedestrian or no pedestrian classifier. Slide it over all possible locations in the image.\n",
    "\n",
    "Example 2: Web Search Ranking (not really a ranking problem)\n",
    "* Classifier takes a query-webpage pair and returns whether the page is relevant or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Classifier\n",
    "\n",
    "* A linear classifier. WX + b = Y\n",
    "* X: inputs\n",
    "* W: Weights (matrix)\n",
    "* b: bias\n",
    "\n",
    "Q: What is the result of 'training' this model?\n",
    "A: Finding the weights and biases that can make accurate predictions.\n",
    "\n",
    "Use a **softmax** function to turn scores into probabilites that sum to 1. These probabilities will be large when scores are large, and small when scores are small.\n",
    "* Each image can have precisely one label, so we want to convert the scores to probabilities an image has a particular label.\n",
    "\n",
    "Scores in logistical regression are also called **logits**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8360188   0.11314284  0.05083836]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEACAYAAACuzv3DAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103897f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"Softmax: Turning scores into probabilities.\"\"\"\n",
    "\n",
    "scores =  [3.0, 1.0, 0.2]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for x.\"\"\"\n",
    "    return np.exp(x)/np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# One row for each score and n columns, \n",
    "# where n is the number of samples.\n",
    "\n",
    "print(softmax(scores))\n",
    "\n",
    "# Plot softmax curves\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# x-coordinates of points to plot\n",
    "x = np.arange(-2.0, 6.0, 0.1)\n",
    "# Create three arrays: x and two arrays shaped like x, one with \n",
    "# entries that are all ones, one with entries that are all 0.2.\n",
    "# Stack arrays in sequence vertically (row-wise) with np.vstack.\n",
    "scores = np.vstack([x, np.ones_like(x), 0.2 *np.ones_like(x)])\n",
    "\n",
    "# .T is transpose\n",
    "plt.plot(x, softmax(scores/1).T, linewidth=2)\n",
    "# plt.label\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      "  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Check that softmax probabilities sum to 1 for every value of x\"\"\"\n",
    "\n",
    "print(np.sum(softmax(scores), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.35448489e-14   2.54281263e-13   6.91208136e-13   1.87889852e-12\n",
      "    5.10737569e-12   1.38832865e-11   3.77386855e-11   1.02584383e-10\n",
      "    2.78853264e-10   7.58001761e-10   2.06046241e-09   5.60091751e-09\n",
      "    1.52248721e-08   4.13854922e-08   1.12497423e-07   3.05799643e-07\n",
      "    8.31249175e-07   2.25956630e-06   6.14211417e-06   1.66958211e-05\n",
      "    4.53826452e-05   1.23353201e-04   3.35237708e-04   9.10745952e-04\n",
      "    2.47179601e-03   6.69062149e-03   1.79802867e-02   4.74107229e-02\n",
      "    1.19167711e-01   2.68875482e-01   4.99916148e-01   7.30992629e-01\n",
      "    8.80761858e-01   9.52558972e-01   9.82007865e-01   9.93304919e-01\n",
      "    9.97526549e-01   9.99088643e-01   9.99664537e-01   9.99876564e-01\n",
      "    9.99954587e-01   9.99983293e-01   9.99993854e-01   9.99997739e-01\n",
      "    9.99999168e-01   9.99999694e-01   9.99999887e-01   9.99999959e-01\n",
      "    9.99999985e-01   9.99999994e-01   9.99999998e-01   9.99999999e-01\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00\n",
      "    1.00000000e+00   1.00000000e+00   1.00000000e+00   1.00000000e+00]\n",
      " [  9.99664650e-01   9.99664650e-01   9.99664650e-01   9.99664650e-01\n",
      "    9.99664650e-01   9.99664650e-01   9.99664650e-01   9.99664650e-01\n",
      "    9.99664650e-01   9.99664649e-01   9.99664648e-01   9.99664644e-01\n",
      "    9.99664635e-01   9.99664608e-01   9.99664537e-01   9.99664344e-01\n",
      "    9.99663819e-01   9.99662391e-01   9.99658510e-01   9.99647960e-01\n",
      "    9.99619282e-01   9.99541338e-01   9.99329525e-01   9.98754209e-01\n",
      "    9.97193683e-01   9.92976272e-01   9.81690393e-01   9.52269826e-01\n",
      "    8.80536902e-01   7.30879336e-01   4.99916148e-01   2.68917160e-01\n",
      "    1.19198156e-01   4.74251187e-02   1.79861014e-02   6.69283590e-03\n",
      "    2.47262111e-03   9.11050916e-04   3.35350093e-04   1.23394571e-04\n",
      "    4.53978680e-05   1.67014218e-05   6.14417459e-06   2.26032430e-06\n",
      "    8.31528027e-07   3.05902227e-07   1.12535162e-07   4.13993755e-08\n",
      "    1.52299795e-08   5.60279641e-09   2.06115362e-09   7.58256042e-10\n",
      "    2.78946809e-10   1.02618796e-10   3.77513454e-11   1.38879439e-11\n",
      "    5.10908903e-12   1.87952882e-12   6.91440011e-13   2.54366565e-13\n",
      "    9.35762297e-14   3.44247711e-14   1.26641655e-14   4.65888615e-15\n",
      "    1.71390843e-15   6.30511676e-16   2.31952283e-16   8.53304763e-17\n",
      "    3.13913279e-17   1.15482242e-17   4.24835426e-18   1.56288219e-18\n",
      "    5.74952226e-19   2.11513104e-19   7.78113224e-20   2.86251858e-20\n",
      "    1.05306174e-20   3.87399763e-21   1.42516408e-21   5.24288566e-22]\n",
      " [  3.35350130e-04   3.35350130e-04   3.35350130e-04   3.35350130e-04\n",
      "    3.35350130e-04   3.35350130e-04   3.35350130e-04   3.35350130e-04\n",
      "    3.35350130e-04   3.35350130e-04   3.35350130e-04   3.35350129e-04\n",
      "    3.35350125e-04   3.35350117e-04   3.35350093e-04   3.35350028e-04\n",
      "    3.35349852e-04   3.35349373e-04   3.35348071e-04   3.35344532e-04\n",
      "    3.35334911e-04   3.35308764e-04   3.35237708e-04   3.35044712e-04\n",
      "    3.34521213e-04   3.33106430e-04   3.29320439e-04   3.19450938e-04\n",
      "    2.95387223e-04   2.45182703e-04   1.67703185e-04   9.02116571e-05\n",
      "    3.99865265e-05   1.59093549e-05   6.03366485e-06   2.24519632e-06\n",
      "    8.29471974e-07   3.05623534e-07   1.12497423e-07   4.13942670e-08\n",
      "    1.52292881e-08   5.60270283e-09   2.06114095e-09   7.58254328e-10\n",
      "    2.78946577e-10   1.02618765e-10   3.77513412e-11   1.38879433e-11\n",
      "    5.10908895e-12   1.87952881e-12   6.91440009e-13   2.54366565e-13\n",
      "    9.35762297e-14   3.44247711e-14   1.26641655e-14   4.65888615e-15\n",
      "    1.71390843e-15   6.30511676e-16   2.31952283e-16   8.53304763e-17\n",
      "    3.13913279e-17   1.15482242e-17   4.24835426e-18   1.56288219e-18\n",
      "    5.74952226e-19   2.11513104e-19   7.78113224e-20   2.86251858e-20\n",
      "    1.05306174e-20   3.87399763e-21   1.42516408e-21   5.24288566e-22\n",
      "    1.92874985e-22   7.09547416e-23   2.61027907e-23   9.60268005e-24\n",
      "    3.53262857e-24   1.29958143e-24   4.78089288e-25   1.75879220e-25]]\n",
      "[[ 0.27809175  0.28010376  0.28212465  0.28415436  0.28619285  0.28824007\n",
      "   0.29029598  0.29236054  0.29443368  0.29651537  0.29860555  0.30070416\n",
      "   0.30281115  0.30492647  0.30705006  0.30918186  0.31132181  0.31346985\n",
      "   0.31562592  0.31778995  0.31996189  0.32214165  0.32432919  0.32652443\n",
      "   0.32872729  0.33093772  0.33315563  0.33538095  0.33761361  0.33985354\n",
      "   0.34210065  0.34435487  0.34661611  0.34888431  0.35115937  0.35344121\n",
      "   0.35572975  0.3580249   0.36032658  0.3626347   0.36494917  0.3672699\n",
      "   0.36959679  0.37192976  0.37426872  0.37661357  0.3789642   0.38132054\n",
      "   0.38368248  0.38604991  0.38842275  0.39080089  0.39318423  0.39557266\n",
      "   0.39796609  0.40036441  0.40276751  0.40517529  0.40758764  0.41000445\n",
      "   0.41242562  0.41485103  0.41728058  0.41971414  0.42215162  0.4245929\n",
      "   0.42703786  0.42948639  0.43193838  0.4343937   0.43685226  0.43931391\n",
      "   0.44177856  0.44424608  0.44671635  0.44918926  0.45166468  0.45414249\n",
      "   0.45662258  0.45910482]\n",
      " [ 0.3753846   0.37433837  0.37328753  0.3722321   0.37117211  0.37010758\n",
      "   0.36903852  0.36796498  0.36688696  0.36580451  0.36471764  0.36362638\n",
      "   0.36253077  0.36143083  0.36032658  0.35921807  0.35810532  0.35698836\n",
      "   0.35586723  0.35474195  0.35361257  0.35247911  0.35134162  0.35020012\n",
      "   0.34905465  0.34790526  0.34675197  0.34559482  0.34443386  0.34326912\n",
      "   0.34210065  0.34092848  0.33975266  0.33857322  0.33739021  0.33620368\n",
      "   0.33501366  0.33382021  0.33262336  0.33142316  0.33021966  0.32901291\n",
      "   0.32780295  0.32658983  0.32537359  0.3241543   0.32293199  0.32170672\n",
      "   0.32047854  0.3192475   0.31801365  0.31677704  0.31553773  0.31429577\n",
      "   0.31305122  0.31180412  0.31055453  0.30930251  0.30804811  0.30679139\n",
      "   0.30553241  0.30427123  0.30300789  0.30174246  0.300475    0.29920556\n",
      "   0.29793421  0.29666099  0.29538599  0.29410924  0.29283082  0.29155079\n",
      "   0.2902692   0.28898611  0.2877016   0.28641572  0.28512852  0.28384009\n",
      "   0.28255047  0.28125973]\n",
      " [ 0.34652366  0.34555787  0.34458782  0.34361354  0.34263504  0.34165235\n",
      "   0.34066549  0.33967448  0.33867935  0.33768012  0.33667681  0.33566946\n",
      "   0.33465808  0.3336427   0.33262336  0.33160007  0.33057287  0.32954179\n",
      "   0.32850686  0.3274681   0.32642554  0.32537923  0.32432919  0.32327545\n",
      "   0.32221806  0.32115703  0.32009241  0.31902423  0.31795253  0.31687734\n",
      "   0.3157987   0.31471665  0.31363123  0.31254247  0.31145042  0.31035511\n",
      "   0.30925659  0.30815489  0.30705006  0.30594214  0.30483117  0.30371719\n",
      "   0.30260026  0.30148041  0.30035768  0.29923213  0.2981038   0.29697274\n",
      "   0.29583898  0.29470259  0.2935636   0.29242207  0.29127804  0.29013157\n",
      "   0.28898269  0.28783148  0.28667796  0.2855222   0.28436425  0.28320415\n",
      "   0.28204197  0.28087774  0.27971154  0.2785434   0.27737338  0.27620154\n",
      "   0.27502793  0.27385261  0.27267563  0.27149705  0.27031692  0.2691353\n",
      "   0.26795224  0.26676781  0.26558205  0.26439503  0.2632068   0.26201742\n",
      "   0.26082696  0.25963545]]\n"
     ]
    }
   ],
   "source": [
    "# Q1: When you multiply scores by 10, what happens?\n",
    "print(softmax(scores * 10))\n",
    "\n",
    "# Q2: When you divide scores by 10, what happens?\n",
    "print(softmax(scores / 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A1: Probabilities get close to 0.0 or 1.0. \n",
    "# -> increasing size of output makes your classifier more confident.\n",
    "# A2: Probabilites get close to a uniform distribution.\n",
    "# -> decreasing size of output makes your classifier more uncertain.\n",
    "\n",
    "# Bear in mind we don't want our classifier to be too confident \n",
    "# initially. We want it to become more confident as it learns with\n",
    "# time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Represent labels mathematically with **one-hot encoding**.\n",
    "* We want the probabilities for the correct class to be close to 1 and the probabilities for incorrect class(es) to be close to 0.\n",
    "* Each label represented by vector as long as their classes. Has value 1.0 for the correct class and zero everywhere else.\n",
    "\n",
    "## Q: Confused as to whether you can swap a & c in the one-hot encoding quiz.\n",
    "\n",
    "One-hot encoding works well until you have tens of thousands or even millions of classes. Then you will have long vectors that contain mostly zeros, which is inefficient. We will deal with this problem later with **embeddings**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy\n",
    "\n",
    "Measure how we're doing by comparing S(Y) and one-hot encoding that corresponds to Y. Only used during training.\n",
    "\n",
    "Measure distance between the two probability vectors using **cross entropy D(S,L)**. \n",
    "\n",
    "$$D(S,L) = \\sum_i L_ilog(S_i)$$\n",
    "\n",
    "* D stands for distance.\n",
    "* S is softmax.\n",
    "* L is one-hot encoding labels.\n",
    "* D(S,L) is not symmetric. \n",
    "* Note log in expression. Softmax guarantees there are no zero probabilities, so you never take a log of zero.\n",
    "\n",
    "![cross_entropy](./images/cross_entropy.png)\n",
    "\n",
    "## Process outline: Multinomial Logistic Classification\n",
    "* Turn input into logits using a linear model (multiply by weight matrix and add a bias).\n",
    "* Feed logits (scores) into softmax to turn them into probabilities.\n",
    "* Compare softmax probabilities to one-hot encoded labels using cross-entropy.\n",
    "\n",
    "![multinomial_logistic_regression](./images/multinomial_logistic_regression.png)\n",
    "\n",
    "Q: How do we find the weights and bias b to have a low distance for the correct class and a high distance for the incorrect class?\n",
    "\n",
    "A: Can measure distance averaged over the entire training set. This is called the **training loss** or **cost**. We want to **minimise the loss**.\n",
    "\n",
    "We've turned a machine learning problem into a numerical optimisation problem. We usually solve this optimisation problem using **gradient descent**.\n",
    "\n",
    "![gradient_descent](./images/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two remaining obstacles:\n",
    "1. Feeding image pixels into optimiser\n",
    "2. How to initialise optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Stability\n",
    "Numerical computations: need to care about calculating values that are too large or too small.\n",
    "\n",
    "E.g. Adding small values to very large values can introduce large errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95367431640625\n"
     ]
    }
   ],
   "source": [
    "# This /should/ return 1.0\n",
    "z = 10**9\n",
    "for i in range(10**6):\n",
    "    z += 10**(-6)\n",
    "z -= 10**9\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999177334\n"
     ]
    }
   ],
   "source": [
    "# Compare the error here where we don't have large \n",
    "# numbers to the error in the previous case.\n",
    "z = 1\n",
    "for i in range(10**6):\n",
    "    z += 10**(-6)\n",
    "z -= 1\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want variables with **mean 0 and equal variance** in our optimisation problems. Union variance (var = 1) is nice.\n",
    "* Non-adherence to this can result in a **badly conditioned** (vs well-conditioned) problem: Optimiser needs to do lots of searching to find a good solution.\n",
    "\n",
    "Example: Normaliastion with images (R - 128)/128.\n",
    "\n",
    "### Initialising weight and biases\n",
    "Draw weights randomly from a Gaussian distribution N(0, sigma), where sigma is small.\n",
    "* Large sigma: Distribution has extreme peak. \n",
    "* Small sigma: Distribution is uncertain. Better to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning is about scaling e.g. this linear model. You will notice it runs slowly even when we are using a linear model on a small amount of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "See ``1. nmist`` notebook in TensorFlow directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training, validation and testing sets.\n",
    "\n",
    "## Measuring performance\n",
    "\n",
    "More subtle than you think. \n",
    "\n",
    "Problem if you have no test set: classifier may only memorise a training set and not generalise.\n",
    "\n",
    "Problem if you have a test set but no validation set: In choosing parameters you gave a tiny bit of information to your classifier. So data from the validation set will bleed into your training set, but that's okay because you have test set you can rely on to measure your real performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set Size\n",
    "The bigger your test size, the less noisy the accuracy measurement will be.\n",
    "Rule of thumb: A change that affects 30 examples is usually statistically significant and can be trusted. This heuristic only applies if classes are approximately equally distributed (i.e. important classes are not rare).\n",
    "\n",
    "People therefore often hold back over 30,000 examples for their validation set so that their accuracy can be significant to 1 decimal place.\n",
    "\n",
    "30,000 examples can be a lot. An alternative is **cross-validation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on Training Logistic Regression\n",
    "\n",
    "Training logistic regression using gradient descent is great: for one, you're directly optimising the error measure you care about. A lot of ML research is therefore about **designing the right loss functions to optimise**.\n",
    "\n",
    "BUT **gradient descent is difficult to scale**:\n",
    "* Loss function incorporates all datapoints.\n",
    "* Calculating the gradient takes 3x the computations for the loss function.\n",
    "* This process is iterative, so we have to do this tens or even hundreds of times.\n",
    "\n",
    "Instead of using all the data, we're going to use a tiny random sample of the data, say of size 1-1000. This seems like it will be a horrible estimator. Sometimes an iteration will get the direction of gradient descent completely wrong and increase the loss instead of decreasing it. We will compensate for this by increasing the number of iterations.\n",
    "\n",
    "This is called **stochastic gradient descent**. This scales well with both data and model size (it's the only method that's fast enough), and so is at the core of deep learning even though it has issues.\n",
    "\n",
    "### Things to Know when implementing SGD\n",
    "\n",
    "1. Inputs need to have mean 0 and equal (small) variance.\n",
    "2. Initial weights need to be random, have mean 0 and equal (small) variance.\n",
    "3. **Momentum**: During each step, we're taking a tiny step in a random direction. On aggregate, these steps take us toward the minimum of the loss. We can take the knowledge accumulated from previous steps to inform us about where we should be heading. We can do this by keeping a running average of the gradient and using that instead of the direction of the current batch of the data. (e.g. M <- 0.9M + derivative of current batch) This often leads to better convergence. \n",
    "4. **Learning rate decay**: With SGD, we're taking smaller, noisier steps towards our objective. So what should the **size of steps** be? (This is a whole area of research.) It turns out that it's always beneficial to make steps smaller and smaller as you train. E.g. you can use exponential decay or make steps smaller when the loss plateaus.\n",
    "\n",
    "### Learning rate tuning\n",
    "Lower learning rate can often make learning faster.\n",
    "\n",
    "SGD's reputation as 'Black Magic'\n",
    "\n",
    "Many hyperparameters:\n",
    "* Initial learning rate\n",
    "* Learning rate decay\n",
    "* Momentum\n",
    "* Batch size\n",
    "* Weight initialisation\n",
    "\n",
    "Tip: When things don't work, try to lower your learning rate first.\n",
    "\n",
    "Alternative approach: **ADAGRAD** is a modification of SGD that implicitly takes care of momentum and learning rate decacy for you. This makes learning less sensitive to hyperparameters and tends to be slightly worse than wewll-optimised SGD but is decent."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
